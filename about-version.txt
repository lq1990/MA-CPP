RNN-01
	naive RNN with 1 hidden layer

RNN-v2-OneHidden
	+ Multi-thread CPU Computing
RNN-v2
	naive RNN with 2 hidden layers
	+ Multi-thread

RNN-v3
	Cuda trial of naive RNN with thrust::device_vector<float>()
RNN-v3.1
	Cuda trial of naive RNN with float* to replace thrust
RNN-v3.2
	use cudaMalloc(Host) instead of cudaMallocManaged
	to speed up

RNN-v4
	based on RNN-v2,
	naive RNN => LSTM
RNN-v4.1
	LSTM, coding that leart from Web
RNN-v4.2 
	encapsulate hidden layer of LSTM, so that multi-hidden layer is easy to new
	1hidden: 使用threadPool会比不用 快8%. 另外：修改为HiddenLayer.hiddenBackward()中只对所有final time step计算lambda, 提速50%.
	
	当前1hidden即可

RNN-v4.3 
	修改了lambda的计算放置位置，现在不在lossFun中而是在它外层函数。
	优点：使得不是每个线程都会加惩罚，真正的惩罚系数和线程数无关; 减少lossFun代码量.










	